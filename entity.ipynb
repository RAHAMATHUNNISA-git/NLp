{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named-entity recognition (NER)\n",
        "(also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes\n",
        "\n",
        "spaCy has an **'ner'** pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the `ents` property of a `Doc` object.\n",
        "\n",
        "https://spacy.io/usage/training#ner\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "b9Yp6yMAgUpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9BxRgtJfeNp1"
      },
      "outputs": [],
      "source": [
        "#perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function to display basic entity info:\n",
        "def show_ents(doc):\n",
        "    if doc.ents:\n",
        "          for ent in doc.ents:\n",
        "               print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
        "    else:\n",
        "        print('No named entities found.')"
      ],
      "metadata": {
        "id": "emqMNFGFgrOP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Hi, everyone in Fahad Hussain cs')"
      ],
      "metadata": {
        "id": "Sra4VccZhjfs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY2g5u1xhjkp",
        "outputId": "ec38be35-1f07-4ed0-8f8d-07a94977f437"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fahad Hussain - GPE - Countries, cities, states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(u'May I go to India or Pakistan, next month to see the virus report?')"
      ],
      "metadata": {
        "id": "3uJ_h6gfhjpJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86zhcBD3hjtf",
        "outputId": "11ada858-df5f-41a0-9940-8728b2f69f33"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India - GPE - Countries, cities, states\n",
            "Pakistan - GPE - Countries, cities, states\n",
            "next month - DATE - Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"## Entity annotations\n",
        "`Doc.ents` are token spans with their own set of annotations.\n",
        "<table>\n",
        "<tr><td>`ent.text`</td><td>The original entity text</td></tr>\n",
        "<tr><td>`ent.label`</td><td>The entity type's hash value</td></tr>\n",
        "<tr><td>`ent.label_`</td><td>The entity type's string description</td></tr>\n",
        "<tr><td>`ent.start`</td><td>The token span's *start* index position in the Doc</td></tr>\n",
        "<tr><td>`ent.end`</td><td>The token span's *stop* index position in the Doc</td></tr>\n",
        "<tr><td>`ent.start_char`</td><td>The entity text's *start* index position in the Doc</td></tr>\n",
        "<tr><td>`ent.end_char`</td><td>The entity text's *stop* index position in the Doc</td></tr>\n",
        "</table>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0uHeiHAZmnt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Can I please borrow 500 dollarss from jhon to buy some Microsoft stock? ')"
      ],
      "metadata": {
        "id": "XOJwQAgMhjwx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJP1hY0FnOPj",
        "outputId": "93b90a38-cce0-41e1-9c03-2f6bd84c3a49"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 4 5 20 23 CARDINAL\n",
            "Microsoft 11 12 55 64 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"## NER Tags\n",
        "Tags are accessible through the `.label_` property of an entity.\n",
        "<table>\n",
        "<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n",
        "<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\n",
        "<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\n",
        "<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\n",
        "<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\n",
        "<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\n",
        "<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\n",
        "<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\n",
        "<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\n",
        "<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\n",
        "<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\n",
        "<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\n",
        "<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\n",
        "<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\n",
        "<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\n",
        "<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\n",
        "<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\n",
        "<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\n",
        "<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "imUj6pjPn1wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## Adding a Named Entity to a Span\n",
        "Normally we would have spaCy build a library of named entities by training it on several samples of text.<br>In this case, we only want to add one value:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JMqZhONtoPCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Fahad to build a U.K. factory for $6 million')"
      ],
      "metadata": {
        "id": "h8WOUUQYnqcN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E35-I-u2nqga",
        "outputId": "9cbb5ac1-fda2-4dfc-f6a5-9aca1f0dba7f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U.K. - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "BQ-ZNh-GnqkF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the hash value of the ORG entity label\n",
        "ORG = doc.vocab.strings[u'PERSON']"
      ],
      "metadata": {
        "id": "Op_hRzLBopXz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a span for the new entity\n",
        "new_ent = Span(doc, 0, 1, label=ORG)"
      ],
      "metadata": {
        "id": "K5y72i1Fo5Fh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add the entity to the existing Doc object\n",
        "doc.ents = list(doc.ents) + [new_ent]"
      ],
      "metadata": {
        "id": "icVtXmVxpFy7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"<font color=green>In the code above, the arguments passed to `Span()` are:</font>\n",
        "-  `doc` - the name of the Doc object\n",
        "-  `0` - the *start* index position of the span\n",
        "-  `1` - the *stop* index position (exclusive)\n",
        "-  `label=PERSON` - the label assigned to our entity\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ymj3IIH1pXj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_xFxODjpU9c",
        "outputId": "d017fe6d-2027-4d8b-d793-680c6947a392"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fahad - PERSON - People, including fictional\n",
            "U.K. - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"___\n",
        "## Adding Named Entities to All Matching Spans\n",
        "What if we want to tag *all* occurrences of \"WORDS\"? WE NEED TO use the PhraseMatcher to identify a series of spans in the Doc:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GJVBaSi2p4Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Our company plsans to introduce a new vacuum cleaner.'\n",
        "          u'If successful, the vacuum cleaner will be our first product.')"
      ],
      "metadata": {
        "id": "0_roFLIHp5OY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRZ2RGFdpcKC",
        "outputId": "0c79dc27-46ca-41d0-c32a-c69f5e098b51"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first - ORDINAL - \"first\", \"second\", etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import pharsematcher and create a matcher object:\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Note: also fixed typo in \"en_core\" to \"en_core\"\n",
        "from spacy.matcher import PhraseMatcher  # Correct spelling\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "AOG61fXPqvsz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the desired pharse patterns:\n",
        "pharse_list = ['vacuum cleaner', 'vacuum-cleaner']\n",
        "pharse_patterns = [nlp(text) for text in pharse_list]"
      ],
      "metadata": {
        "id": "KiMWiLiisfAf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher.add('newproduct', None, *pharse_patterns)"
      ],
      "metadata": {
        "id": "oikroso6tpvZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the patterns to our matcher object:\n",
        "matches = matcher(doc)"
      ],
      "metadata": {
        "id": "BWrQQsKgsfEO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#see what matches occur:\n",
        "matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_QgKm04sfIR",
        "outputId": "19ecc8c0-72c7-4e11-b2ee-0452c9671306"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2689272359382549672, 7, 9), (2689272359382549672, 14, 16)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "3DsTMduptgdw"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROD = doc.vocab.strings[u'PRODUCT']"
      ],
      "metadata": {
        "id": "FOhGUo3Ft8Rk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ents = [Span(doc, match[1],match[2],label=PROD) for match in matches]"
      ],
      "metadata": {
        "id": "gqw5Uo1zuEi9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2-EKNNDuasS",
        "outputId": "956434a4-31be-49a3-fe11-6e5f269b38da"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first - ORDINAL - \"first\", \"second\", etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"___\n",
        "## Counting Entities\n",
        "While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c6Eh6lsKujbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')"
      ],
      "metadata": {
        "id": "PqG3vwCvuawT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icyx99Uduaze",
        "outputId": "137e817d-3f9e-438a-9724-69da796fe3ad"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.50 - MONEY - Monetary values, including unit\n",
            "five dollars - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len([ent for ent in doc.ents if ent.label_=='MONEY'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4XKLBQTu7pD",
        "outputId": "65c44864-4623-4cfc-cb1f-439b797ad2a0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')"
      ],
      "metadata": {
        "id": "opzvPfGVvOG8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo33BK_vv19",
        "outputId": "02b5f82b-76dc-450d-e931-6f14d46b72f1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.50 - MONEY - Monetary values, including unit\n",
            "five dollars - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"\"\"### <font color=blue>However, there is a simple fix that can be added to the nlp pipeline:</font>\n",
        "\n",
        "https://spacy.io/usage/processing-pipelines\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "96bg8baGwrIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Quick function to remove ents formed on whitespace:\n",
        "def remove_whitespace_entities(doc):\n",
        "  doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
        "  return doc"
      ],
      "metadata": {
        "id": "V6p3m57bwngn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Step 1: Define and register the component with a name\n",
        "@Language.component(\"remove_whitespace_entities\")  # Register with a name\n",
        "def remove_whitespace_entities(doc):\n",
        "    # Filter out entities that are just whitespace\n",
        "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
        "    return doc\n",
        "\n",
        "# Step 2: Load the model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # or any other model\n",
        "\n",
        "# Step 3: Add to pipeline USING THE REGISTERED NAME (as a string!)\n",
        "nlp.add_pipe(\"remove_whitespace_entities\", after=\"ner\")  # âœ… Correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "_Emy0ApLzY2D",
        "outputId": "16c1f57b-6c8c-4fd6-dcce-a95094aabbda"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.remove_whitespace_entities(doc)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>remove_whitespace_entities</b><br/>def remove_whitespace_entities(doc)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-66-a6f27bfb1f34&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language\n",
        "\n",
        "# Manually register the component (instead of using @Language.component)\n",
        "Language.component(\"remove_whitespace_entities\", func=remove_whitespace_entities)\n",
        "\n",
        "# Then add it to the pipeline\n",
        "nlp.add_pipe(\"remove_whitespace_entities\", after=\"ner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "xZ7k2UBpzY6g",
        "outputId": "6dcb1d9f-fed2-42da-ceb7-bdd9c10b1a5c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E007] 'remove_whitespace_entities' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'remove_whitespace_entities']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-96aa58d51d30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Then add it to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"remove_whitespace_entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;31m# Overriding pipe name in the config is not supported and will be ignored.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E007] 'remove_whitespace_entities' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'remove_whitespace_entities']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.pipe_names)  # Should show ['tok2vec', 'tagger', 'parser', 'ner', 'remove_whitespace_entities']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3XKRJN2zoaT",
        "outputId": "5e5588f4-2e28-4ec6-fdd7-b34cc6499d10"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'remove_whitespace_entities']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert this into the pipeline AFTER the ner component:\n",
        "nlp.add_pipe(remove_whitespace_entities, after='ner')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "VfYXqlc4z480",
        "outputId": "ba118380-e072-4fdc-f66e-8645178a45df"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function remove_whitespace_entities at 0x7c31f55afd80> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-3787b5446c85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Insert this into the pipeline AFTER the ner component:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_whitespace_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function remove_whitespace_entities at 0x7c31f55afd80> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rerun nlp on the text above, and show ents:\n",
        "doc = nlp(u'Originally priced at $29.50,\\nthe sweater was marked down to five dollars.')\n"
      ],
      "metadata": {
        "id": "KzQAT9oQz5A-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBhvLKzF0Nwb",
        "outputId": "bdb55e47-c19f-41c4-f1fb-f673d99d3051"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.50 - MONEY - Monetary values, including unit\n",
            "five dollars - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"For more on **Named Entity Recognition** visit https://spacy.io/usage/linguistic-features#101\n",
        "\n",
        "___\n",
        "## Noun Chunks\n",
        "`Doc.noun_chunks` are *base noun phrases*: token spans that include the noun and words describing the noun. Noun chunks cannot be nested, cannot overlap, and do not involve prepositional phrases or relative clauses.<br>\n",
        "Where `Doc.ents` rely on the **ner** pipeline component, `Doc.noun_chunks` are provided by the **parser**.\n",
        "\n",
        "### `noun_chunks` components:\n",
        "<table>\n",
        "<tr><td>`.text`</td><td>The original noun chunk text.</td></tr>\n",
        "<tr><td>`.root.text`</td><td>The original text of the word connecting the noun chunk to the rest of the parse.</td></tr>\n",
        "<tr><td>`.root.dep_`</td><td>Dependency relation connecting the root to its head.</td></tr>\n",
        "<tr><td>`.root.head.text`</td><td>The text of the root token's head.</td></tr>\n",
        "</table>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rxi8mlbF0U35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"Auntonomous cars shift insuranceliability toward manufactures.\")"
      ],
      "metadata": {
        "id": "_cTiutz80N0Y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "  print(chunk.text+'-'+chunk.root.text+'-'+chunk.root.dep_+'-'+chunk.root.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2ayV8610N3l",
        "outputId": "a5965d80-a52a-4d0e-dba8-e7b8dbef55b6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auntonomous cars-cars-nsubj-shift\n",
            "insuranceliability-insuranceliability-dobj-shift\n",
            "manufactures-manufactures-pobj-toward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"### `Doc.noun_chunks` is a  generator function\n",
        "Previously we mentioned that `Doc` objects do not retain a list of sentences, but they're available through the `Doc.sents` generator.<br>It's the same with `Doc.noun_chunks` - lists can be created if needed:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uH00fqyQ6GLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(doc.noun_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUJYb1h16Fx5",
        "outputId": "703e1605-6190-4593-c37a-111bd9603a17"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"For more on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks\"\"\""
      ],
      "metadata": {
        "id": "-p7S_pRd7YeF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbEvb0Th6F1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}